{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM(거대 언어 모델)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP(자연어 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이징 결과:  [['자연어', '처리', '는', '재미있는', '분야', '입니다', '.'], ['딥', '러닝', '은', '많은', '데이터', '를', '필요', '로', '합니다', '.'], ['한국어', 'NLP', '는', '정말', '재미있어요', '!']]\n"
     ]
    }
   ],
   "source": [
    "#사전 설치 :pip install konlpy\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#텍스트 데이터 (입력문장)\n",
    "sentences=[\n",
    "    \"자연어 처리는 재미있는 분야입니다.\",\n",
    "    \"딥러닝은 많은 데이터를 필요로 합니다.\",\n",
    "    \"한국어 NLP는 정말 재미있어요!\"\n",
    "];\n",
    "\n",
    "#토크나이징\n",
    "okt=Okt();\n",
    "tokenized_sentences=[okt.morphs(sentence) for sentence in sentences];\n",
    "print(\"토크나이징 결과: \", tokenized_sentences);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 결과: [[3, 4, 1, 5, 6, 7, 2], [8, 9, 10, 11, 12, 13, 14, 15, 16, 2], [17, 18, 1, 19, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "#인코딩: 단어를 숫자로 변환\n",
    "tokenizer=Tokenizer();\n",
    "tokenizer.fit_on_texts(tokenized_sentences);\n",
    "encoded_sentences=tokenizer.texts_to_sequences(tokenized_sentences);\n",
    "print(\"인코딩 결과:\", encoded_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과: [[ 3  4  1  5  6  7  2  0  0  0]\n",
      " [ 8  9 10 11 12 13 14 15 16  2]\n",
      " [17 18  1 19 20 21  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#패딩(padding): 길이를 맞추기 위해 0으로 채우기\n",
    "max_len=10  #최대길이 설정\n",
    "padded_sentences=pad_sequences(encoded_sentences, maxlen=max_len, padding=\"post\");\n",
    "print(\"패딩 결과:\", padded_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임베딩(Embedding)\n",
    "vocab_size=len(tokenizer.word_index)+1  #단어 사전 크기\n",
    "embedding_dim=8     #임베딩 차원 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#간단한 임베딩 모델 생성\n",
    "model=Sequential();\n",
    "model.add (Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len));\n",
    "model.compile(\"rmsprop\", \"mse\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "임베딩 결과(첫번째 문장):\n",
      " [[ 0.03785     0.02770555  0.01835119  0.01561788  0.03928561  0.00935874\n",
      "   0.01045211  0.0094878 ]\n",
      " [ 0.03607792 -0.01949482 -0.04625165 -0.04639664  0.00307175 -0.04118716\n",
      "  -0.006867    0.00686441]\n",
      " [ 0.02437948 -0.02414671  0.04831756  0.04589019 -0.01767214 -0.02486076\n",
      "  -0.02759489 -0.02554921]\n",
      " [ 0.02879046  0.01846165  0.03845319 -0.00784381 -0.00483923 -0.02904035\n",
      "   0.01024349 -0.01850778]\n",
      " [ 0.01767942  0.02069113  0.01443192 -0.00825565  0.00944923  0.0015115\n",
      "   0.0319241   0.04393909]\n",
      " [-0.01904782 -0.01613797 -0.00510596  0.00372597 -0.0250793  -0.03565183\n",
      "   0.01453916  0.03772693]\n",
      " [ 0.0078523  -0.04798561 -0.01991482  0.03380628 -0.02201792  0.04673639\n",
      "   0.0421188   0.02592966]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]]\n",
      "\n",
      " [[ 0.03395226 -0.00432321 -0.04299065  0.01464262  0.01371543  0.01401154\n",
      "  -0.0053302  -0.01480514]\n",
      " [-0.02359686  0.03903845  0.04548376 -0.0272112  -0.01327753  0.00574439\n",
      "  -0.01327888 -0.04019899]\n",
      " [-0.00341725  0.02385608 -0.04429914  0.00536615 -0.0455589   0.02593981\n",
      "  -0.00537897  0.02171686]\n",
      " [ 0.01310773  0.02589924 -0.03995325 -0.01319597  0.01401483  0.03621611\n",
      "  -0.04967473  0.01306751]\n",
      " [ 0.01251029 -0.04375069  0.00949559  0.00067496  0.03592953 -0.0498229\n",
      "   0.02225796  0.00831938]\n",
      " [-0.04645608  0.00489121  0.01039397 -0.0326244  -0.00844467 -0.02876142\n",
      "   0.04531423 -0.00432862]\n",
      " [ 0.00595457  0.0236663   0.02245775 -0.04295762  0.02184698 -0.02363321\n",
      "  -0.01317421  0.02282865]\n",
      " [-0.00227156  0.01616453 -0.03327421 -0.00897496 -0.00901385 -0.04305254\n",
      "   0.03046514 -0.00744075]\n",
      " [-0.03320014 -0.03941999  0.03716874 -0.01595331  0.02303023 -0.03706046\n",
      "   0.04917189 -0.02570795]\n",
      " [ 0.0078523  -0.04798561 -0.01991482  0.03380628 -0.02201792  0.04673639\n",
      "   0.0421188   0.02592966]]\n",
      "\n",
      " [[-0.01853626 -0.04593427  0.04864273  0.00583446 -0.04341309 -0.03414873\n",
      "   0.04012319 -0.04500258]\n",
      " [-0.02555904  0.04053709  0.02440285  0.00428222 -0.01665133  0.00510081\n",
      "   0.02711277  0.0414401 ]\n",
      " [ 0.02437948 -0.02414671  0.04831756  0.04589019 -0.01767214 -0.02486076\n",
      "  -0.02759489 -0.02554921]\n",
      " [-0.00202315 -0.03939899  0.01039196  0.04045967 -0.03226788 -0.0202237\n",
      "  -0.0308449  -0.01545011]\n",
      " [-0.00119456 -0.0100494   0.00525091  0.03539253  0.03294568  0.02834808\n",
      "   0.00827155 -0.02233511]\n",
      " [ 0.02292761  0.04311008 -0.01299553  0.00412361 -0.00394046  0.01339874\n",
      "   0.00154886  0.03506032]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]]\n"
     ]
    }
   ],
   "source": [
    "#패딩된 문장을 임베딩 층에 통과\n",
    "embeddings=model.predict(padded_sentences);\n",
    "print(\"임베딩 결과(첫번째 문장):\\n\", embeddings[0]);\n",
    "print(\"\\n\",embeddings[1]);\n",
    "print(\"\\n\", embeddings[2]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머(Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggin Face를 사용한  BERT 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8848785758018494}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정분석(zero-shot classification)\n",
    "### transformers 라이브러리 사전 설치 : pip install transformers\n",
    "### tf-keras 라이브러리 사전 설치치 : pip install tf-keras\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier=pipeline(\"sentiment-analysis\");\n",
    "classifier(\"오늘 기분이 좋아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995144605636597}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use the Google Assistant for the job and how to build productivity apps.\\n\\nHow to Build a Pro-Workout with Google Assistant\\n\\nWe will have three parts for the job:\\n\\n'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텍스트 생성(text generation)\n",
    "generator=pipeline(\"text-generation\");\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-10\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949762105941772, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question-answering\n",
    "question_answer=pipeline(\"question-answering\");\n",
    "question_answer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-10\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#요약(Summarization)\n",
    "summarizer=pipeline(\"summarization\");\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
