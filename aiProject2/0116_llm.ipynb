{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM(거대 언어 모델)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP(자연어 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Okt\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embedding\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#사전 설치 :pip install konlpy\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#텍스트 데이터 (입력문장)\n",
    "sentences=[\n",
    "    \"자연어 처리는 재미있는 분야입니다.\",\n",
    "    \"딥러닝은 많은 데이터를 필요로 합니다.\",\n",
    "    \"한국어 NLP는 정말 재미있어요!\"\n",
    "];\n",
    "\n",
    "#토크나이징\n",
    "okt=Okt();\n",
    "tokenized_sentences=[okt.morphs(sentence) for sentence in sentences];\n",
    "print(\"토크나이징 결과: \", tokenized_sentences);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 결과: [[3, 4, 1, 5, 6, 7, 2], [8, 9, 10, 11, 12, 13, 14, 15, 16, 2], [17, 18, 1, 19, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "#인코딩: 단어를 숫자로 변환\n",
    "tokenizer=Tokenizer();\n",
    "tokenizer.fit_on_texts(tokenized_sentences);\n",
    "encoded_sentences=tokenizer.texts_to_sequences(tokenized_sentences);\n",
    "print(\"인코딩 결과:\", encoded_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과: [[ 3  4  1  5  6  7  2  0  0  0]\n",
      " [ 8  9 10 11 12 13 14 15 16  2]\n",
      " [17 18  1 19 20 21  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#패딩(padding): 길이를 맞추기 위해 0으로 채우기\n",
    "max_len=10  #최대길이 설정\n",
    "padded_sentences=pad_sequences(encoded_sentences, maxlen=max_len, padding=\"post\");\n",
    "print(\"패딩 결과:\", padded_sentences);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#임베딩(Embedding)\n",
    "vocab_size=len(tokenizer.word_index)+1  #단어 사전 크기\n",
    "embedding_dim=8     #임베딩 차원 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#간단한 임베딩 모델 생성\n",
    "model=Sequential();\n",
    "model.add (Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len));\n",
    "model.compile(\"rmsprop\", \"mse\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "임베딩 결과(첫번째 문장):\n",
      " [[ 0.03785     0.02770555  0.01835119  0.01561788  0.03928561  0.00935874\n",
      "   0.01045211  0.0094878 ]\n",
      " [ 0.03607792 -0.01949482 -0.04625165 -0.04639664  0.00307175 -0.04118716\n",
      "  -0.006867    0.00686441]\n",
      " [ 0.02437948 -0.02414671  0.04831756  0.04589019 -0.01767214 -0.02486076\n",
      "  -0.02759489 -0.02554921]\n",
      " [ 0.02879046  0.01846165  0.03845319 -0.00784381 -0.00483923 -0.02904035\n",
      "   0.01024349 -0.01850778]\n",
      " [ 0.01767942  0.02069113  0.01443192 -0.00825565  0.00944923  0.0015115\n",
      "   0.0319241   0.04393909]\n",
      " [-0.01904782 -0.01613797 -0.00510596  0.00372597 -0.0250793  -0.03565183\n",
      "   0.01453916  0.03772693]\n",
      " [ 0.0078523  -0.04798561 -0.01991482  0.03380628 -0.02201792  0.04673639\n",
      "   0.0421188   0.02592966]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]]\n",
      "\n",
      " [[ 0.03395226 -0.00432321 -0.04299065  0.01464262  0.01371543  0.01401154\n",
      "  -0.0053302  -0.01480514]\n",
      " [-0.02359686  0.03903845  0.04548376 -0.0272112  -0.01327753  0.00574439\n",
      "  -0.01327888 -0.04019899]\n",
      " [-0.00341725  0.02385608 -0.04429914  0.00536615 -0.0455589   0.02593981\n",
      "  -0.00537897  0.02171686]\n",
      " [ 0.01310773  0.02589924 -0.03995325 -0.01319597  0.01401483  0.03621611\n",
      "  -0.04967473  0.01306751]\n",
      " [ 0.01251029 -0.04375069  0.00949559  0.00067496  0.03592953 -0.0498229\n",
      "   0.02225796  0.00831938]\n",
      " [-0.04645608  0.00489121  0.01039397 -0.0326244  -0.00844467 -0.02876142\n",
      "   0.04531423 -0.00432862]\n",
      " [ 0.00595457  0.0236663   0.02245775 -0.04295762  0.02184698 -0.02363321\n",
      "  -0.01317421  0.02282865]\n",
      " [-0.00227156  0.01616453 -0.03327421 -0.00897496 -0.00901385 -0.04305254\n",
      "   0.03046514 -0.00744075]\n",
      " [-0.03320014 -0.03941999  0.03716874 -0.01595331  0.02303023 -0.03706046\n",
      "   0.04917189 -0.02570795]\n",
      " [ 0.0078523  -0.04798561 -0.01991482  0.03380628 -0.02201792  0.04673639\n",
      "   0.0421188   0.02592966]]\n",
      "\n",
      " [[-0.01853626 -0.04593427  0.04864273  0.00583446 -0.04341309 -0.03414873\n",
      "   0.04012319 -0.04500258]\n",
      " [-0.02555904  0.04053709  0.02440285  0.00428222 -0.01665133  0.00510081\n",
      "   0.02711277  0.0414401 ]\n",
      " [ 0.02437948 -0.02414671  0.04831756  0.04589019 -0.01767214 -0.02486076\n",
      "  -0.02759489 -0.02554921]\n",
      " [-0.00202315 -0.03939899  0.01039196  0.04045967 -0.03226788 -0.0202237\n",
      "  -0.0308449  -0.01545011]\n",
      " [-0.00119456 -0.0100494   0.00525091  0.03539253  0.03294568  0.02834808\n",
      "   0.00827155 -0.02233511]\n",
      " [ 0.02292761  0.04311008 -0.01299553  0.00412361 -0.00394046  0.01339874\n",
      "   0.00154886  0.03506032]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]\n",
      " [-0.04754175 -0.02237874  0.03572438  0.01527183  0.04651963  0.03915666\n",
      "   0.03639365  0.032008  ]]\n"
     ]
    }
   ],
   "source": [
    "#패딩된 문장을 임베딩 층에 통과\n",
    "embeddings=model.predict(padded_sentences);\n",
    "print(\"임베딩 결과(첫번째 문장):\\n\", embeddings[0]);\n",
    "print(\"\\n\",embeddings[1]);\n",
    "print(\"\\n\", embeddings[2]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트랜스포머(Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggin Face를 사용한  BERT 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.8848785758018494}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정분석(zero-shot classification)\n",
    "### transformers 라이브러리 사전 설치 : pip install transformers\n",
    "### tf-keras 라이브러리 사전 설치치 : pip install tf-keras\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier=pipeline(\"sentiment-analysis\");\n",
    "classifier(\"오늘 기분이 좋아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995144605636597}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use the Google Assistant for the job and how to build productivity apps.\\n\\nHow to Build a Pro-Workout with Google Assistant\\n\\nWe will have three parts for the job:\\n\\n'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텍스트 생성(text generation)\n",
    "generator=pipeline(\"text-generation\");\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-10\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949762105941772, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question-answering\n",
    "question_answer=pipeline(\"question-answering\");\n",
    "question_answer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-10\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#요약(Summarization)\n",
    "summarizer=pipeline(\"summarization\");\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-10\\AppData\\Local\\Temp\\ipykernel_9588\\2523185121.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask'),\n",
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#사전설치: pip install faiss-cpu\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"영준은 랭체인 주식회사에서 근무를 하였습니다.\",\n",
    "        \"설현은 테디와 같은 회사에서 근무하였습니다.\",\n",
    "        \"영준의 직업은 개발자입니다.\",\n",
    "        \"설현의 직업은 디자이너입니다.\",\n",
    "    ],\n",
    "    embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask'),\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever();\n",
    "\n",
    "template=\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question:{question}\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "#model=Ollama(model=\"llama3:8b\");       #meta\n",
    "model=Ollama(model=\"gemma2\");           #google\n",
    "\n",
    "retrieval_chain=(\n",
    "    {\"context\":retriever,\"question\":RunnablePassthrough()}\n",
    "    |prompt\n",
    "    |model\n",
    "    |StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'디자이너입니다. \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"설현의 직업은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'영준은 랭체인 주식회사에서 근무를 하였습니다.  \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"영준이 근무한 곳은?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'죄송합니다만, 주어진 텍스트에서는 영준의 선호하는 SW 기술에 대한 정보가 없습니다.  \\n\\n\\nDocument 분석 결과는 다음과 같습니다. \\n\\n* **영준:** 개발자, 랭체인 주식회사에서 근무 경력\\n* **설현:** 디자이너, 테디와 같은 회사에서 근무 경력\\n\\n\\n개발자가 유망하게 여길 수 있는 SW 기술은 매우 다양하며, 영준의 업무 분야나 관심사에 따라 달라집니다. \\n\\n예를 들어, 웹 개발을 선호하는 경우 JavaScript, React, Node.js 등이 좋고, 모바일 앱 개발을 선호하는 경우 Swift, Kotlin, Flutter 등이 유망할 수 있습니다. 데이터 분석이나 머신러닝 분야에 관심 있다면 Python, R, TensorFlow와 같은 기술을 학습하는 것이 도움이 될 것입니다.\\n\\n\\n더 자세한 정보가 필요하면 영준의 업무 분야나 관심사를 알려주시기 바랍니다. \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"영준이 개발자라면 유망한 SW 기술 추천해줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain+sql예제(대화이력 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "chat_message_history=SQLChatMessageHistory(\n",
    "    session_id=\"sql_caht_history\",\n",
    "    connection_string=\"mysql+pymysql://root:11111111@localhost:3306/test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history.add_user_message(\n",
    "    \"안녕. 나는 영준이야. 직업은 웹프로그래머이고 만나서 반가워!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message_history.add_user_message(\n",
    "    \"요즘 날씨가 추운데 건강 조심하고 즐거운 하루보내!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='안녕. 나는 영준이야. 직업은 웹프로그래머이고 만나서 반가워!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='요즘 날씨가 추운데 건강 조심하고 즐거운 하루보내!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_message_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain+ollama 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install langchain, pip install langchain-community\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm=Ollama(model=\"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 저는 구글에서 훈련된 대규모 언어 모델입니다. 사람들이 질문에 답하고, 이야기를 만들고, 다양한 작업을 수행할 수 있도록 도와줍니다.\\n\\n무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"안녕, 간단하게 소개해줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Rag 예제: ollama+gemma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로드\n",
    "loader=TextLoader(\"dataset/history.txt\", encoding=\"UTF8\")\n",
    "documents=loader.load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-10\\AppData\\Local\\Temp\\ipykernel_9588\\3081423701.py:2: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-10\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#벡터 임베딩 생성(Hugging Face 사용)\n",
    "embeddings=HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검색기(retriever) 설정\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ollama Gemma2 모델 초기화\n",
    "llm=Ollama(model=\"gemma2\", base_url=\"http://localhost:11434\")   #Ollama 서버 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 체인 구성\n",
    "qa_chain=RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-10\\AppData\\Local\\Temp\\ipykernel_9588\\782169715.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response=qa_chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 고조선은 기원전 2333년에 단군왕검에 의해 설립되었다고 전해집니다.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#질문 실행\n",
    "query=\"고조선은 언제 설립되었는지 알려줘\"\n",
    "response=qa_chain.run(query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 조선은 이성계에 의해 건국되었다.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"조선은 누가 건국했는지 알려줘\"\n",
    "response=qa_chain.run(query)\n",
    "print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
