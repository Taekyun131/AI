{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탭 나누는 샘플코드\n",
    "import gradio as gr\n",
    "\n",
    "def process_text1(input_text):\n",
    "    return f\"Tab 1에서 입력된 텍스트: {input_text}\"\n",
    "\n",
    "def process_text2(input_text):\n",
    "    return f\"Tab 2에서 입력된 텍스트: {input_text}\"\n",
    "\n",
    "# 각 탭을 구성하는 인터페이스 목록\n",
    "tab1 = gr.Interface(\n",
    "    fn=process_text1, \n",
    "    inputs=gr.Textbox(placeholder=\"Tab 1에 텍스트 입력\", label=\"Tab 1 입력\"), \n",
    "    outputs=gr.Textbox())\n",
    "tab2 = gr.Interface(\n",
    "    fn=process_text2, \n",
    "    inputs=gr.Textbox(placeholder=\"Tab 2에 텍스트 입력\", label=\"Tab 2 입력\"), \n",
    "    outputs=gr.Textbox())\n",
    "\n",
    "# Gradio 인터페이스 정의\n",
    "with gr.Blocks() as demo:\n",
    "    gr.TabbedInterface(interface_list=[tab1, tab2])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\__init__.py:117\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwasm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IS_WASM\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m IS_WASM:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deploy\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mipython_ext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ipython_extension\n\u001b[0;32m    120\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m get_package_version()\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\cli\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cli, deploy\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_component\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcli\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeploy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_component\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\cli\\cli.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deploy_discord  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrich\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconsole\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_component, deploy, print_environment_info, reload\n\u001b[0;32m      9\u001b[0m app \u001b[38;5;241m=\u001b[39m typer\u001b[38;5;241m.\u001b[39mTyper()\n\u001b[0;32m     10\u001b[0m app\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment\u001b[39m\u001b[38;5;124m\"\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrint Gradio environment information.\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m     11\u001b[0m     print_environment_info\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\cli\\commands\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli_env_info\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_environment_info\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m app \u001b[38;5;28;01mas\u001b[39;00m custom_component\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeploy_space\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deploy\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m reload\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\cli\\commands\\components\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m app\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\cli\\commands\\components\\app.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Typer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _build\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _create\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdev\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _dev\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\gradio\\cli\\commands\\components\\build.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msemantic_version\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyper\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump, parse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_component_analytics\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tomlkit\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOMLDocument\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aot\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array\n",
      "File \u001b[1;32md:\\241223\\aiProject2\\.venv\\lib\\site-packages\\tomlkit\\api.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeVar\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_rfc3339\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Container\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnexpectedCharError\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtomlkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mitems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CUSTOM_ENCODERS\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import ollama\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "# 텍스트 파일을 읽어들이는 함수\n",
    "def read_txt(file):\n",
    "    # Gradio의 File 객체는 file.read()로 바이너리 데이터를 반환하므로, 이를 디코딩하여 텍스트로 변환\n",
    "    return file.read().decode('utf-8')\n",
    "\n",
    "# 텍스트 파일을 학습시키고 벡터화하는 함수\n",
    "def process_text(file):\n",
    "    # 텍스트 파일 내용 읽기\n",
    "    file_content = read_txt(file)\n",
    "    \n",
    "    # 문서 로더를 사용해 텍스트를 문서로 변환\n",
    "    loader = TextLoader(file.name)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # SentenceTransformer를 사용해 텍스트를 임베딩\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # FAISS를 사용하여 벡터를 저장\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "    # 벡터스토어 반환\n",
    "    return vector_store\n",
    "\n",
    "# Ollama 모델을 사용하여 텍스트 기반 질문에 답하는 함수\n",
    "def answer_question(file, question):\n",
    "    # 텍스트 파일 처리 및 벡터화\n",
    "    vector_store = process_text(file)\n",
    "    \n",
    "    # FAISS 벡터스토어에서 가장 관련성이 높은 문서를 검색\n",
    "    docs = vector_store.similarity_search(question, k=3)\n",
    "    \n",
    "    # Ollama 모델을 사용하여 질문에 대한 답변 생성\n",
    "    prompt = f\"다음 텍스트를 기반으로 질문에 답변하세요:\\n{docs}\\n질문: {question}\\n답변:\"\n",
    "\n",
    "    try:\n",
    "        # Ollama 모델을 사용하여 답변 생성\n",
    "        response = ollama.chat(model=\"llama2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response['text']\n",
    "    except Exception as e:\n",
    "        return f\"오류 발생: {str(e)}\"\n",
    "\n",
    "# Gradio 인터페이스 설정\n",
    "def create_gradio_interface():\n",
    "    # 텍스트 파일 업로드 입력\n",
    "    file_input = gr.File(label=\"Upload a Text File\")\n",
    "    \n",
    "    # 질문 입력 및 답변 출력\n",
    "    question_input = gr.Textbox(label=\"Ask a Question\", placeholder=\"Enter your question here\")\n",
    "    question_output = gr.Textbox(label=\"Answer\")\n",
    "    \n",
    "    # Gradio 인터페이스 설정\n",
    "    with gr.Blocks() as iface:\n",
    "        gr.Interface(fn=answer_question, inputs=[file_input, question_input], outputs=question_output).launch()\n",
    "\n",
    "    return iface\n",
    "\n",
    "# Gradio 인터페이스 실행\n",
    "if __name__ == \"__main__\":\n",
    "    iface = create_gradio_interface()\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "iface.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "class show_recipe:\n",
    "    def __init__(self):\n",
    "        self.answer_template = \"\"\"\n",
    "        당신은 음식에 대한 조리법을 제공하는 AI 어시스턴트입니다.\n",
    "        다음 정보를 바탕으로 사용자의 질문에 대해 한국어로 답변해주세요:\n",
    "        \n",
    "        아래는 이전 대화 내용입니다:\n",
    "        {chat_history}\n",
    "\n",
    "        관련 문서 내용:\n",
    "        {context}\n",
    "        \n",
    "        사용자 질문: {question}\n",
    "        \n",
    "        규칙:\n",
    "        1. 결과를 자연스러운 한국어로 설명해주세요\n",
    "        2. 모든 답변은 친절하고 전문적으로 제공해주세요.\n",
    "        3. 결과가 없다면 그 이유를 설명해주세요\n",
    "        4. 요리가 익숙하지 않은 사람도 이해하기 쉽게 설명해주세요\n",
    "        5. 조리방법을 번호를 매겨 순서대로 설명해주세요\n",
    "        6. \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Gemma2 모델 초기화\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = Ollama(\n",
    "            model=\"gemma2\",\n",
    "            temperature=0,\n",
    "            callback_manager=callback_manager\n",
    "        )\n",
    "\n",
    "        # 프롬프트 템플릿 설정\n",
    "        self.query_prompt = ChatPromptTemplate.from_template(self.query_template)\n",
    "        self.answer_prompt = ChatPromptTemplate.from_template(self.answer_template)\n",
    "\n",
    "        # Chain 설정\n",
    "        self.query_chain = LLMChain(llm=self.llm, prompt=self.query_prompt)\n",
    "        self.answer_chain = LLMChain(llm=self.llm, prompt=self.answer_prompt)\n",
    "\n",
    "    def generate_query(self, question: str, schema_info: str, feedback_info: str = \"\") -> str:\n",
    "        \"\"\"질문에 대한 SQL 쿼리를 생성합니다.\"\"\"\n",
    "        response = self.query_chain.run(\n",
    "            question=question,\n",
    "            schema_info=schema_info,\n",
    "            feedback_info=feedback_info\n",
    "        )\n",
    "        return self.extract_sql_query(response)\n",
    "\n",
    "    def generate_answer(self, question: str, query: str, result: Any) -> str:\n",
    "        \"\"\"쿼리 결과를 바탕으로 자연어 답변을 생성합니다.\"\"\"\n",
    "        result_str = str(result) if isinstance(result, pd.DataFrame) else json.dumps(result, ensure_ascii=False)\n",
    "        response = self.answer_chain.run(\n",
    "            question=question,\n",
    "            query=query,\n",
    "            result=result_str\n",
    "        )\n",
    "        return response.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
